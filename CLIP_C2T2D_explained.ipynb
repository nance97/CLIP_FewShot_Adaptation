{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQiprE0pcVD"
      },
      "source": [
        "# CLIP zero-shot Evaluation\n",
        "This short notebook implements the dataset split into base and novel categories (see project assignment) and runs the zero-shot evaluation with CLIP.\n",
        "Feel free to copy the code contained in this notebook or to directly use this notebook as starting point for you project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UzXtFjhh7iOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ea7e98-c118-450c-c312-e746e51c53da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai_clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from openai_clip)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from openai_clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai_clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->openai_clip) (0.2.14)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai_clip\n",
            "  Building wheel for openai_clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai_clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=639f9766c36fc108f4a9f20802dd2831386b3766cd1b782cc777df538f6e5b96\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/49/bc/c2342e8e14878210ba4825cf314a53f2570f6fb18b91fce3cf\n",
            "Successfully built openai_clip\n",
            "Installing collected packages: ftfy, openai_clip\n",
            "Successfully installed ftfy-6.3.1 openai_clip-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# we need to install clip as it is not pre-installed\n",
        "# you are also free to use open_clip which provide more models\n",
        "# https://github.com/mlfoundations/open_clip\n",
        "%pip install openai_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QtqdSOr8qqOn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import clip\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.cuda import amp\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2353MHw1p24h"
      },
      "source": [
        "## Dataset Loading\n",
        "Let's get the data directly from torchvision as we have seen during labs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M_1CrUhZpVCq"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\n",
        "    Args:\n",
        "        data_dir (str): Directory where the dataset will be stored.\n",
        "        transform (torch.Compose)\n",
        "    Returns:\n",
        "        tuple: A tuple containing the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJI_a5EizA5a"
      },
      "source": [
        "## Base and Novel categories\n",
        "To split in base and novel categories we list all dataset classes, and count their number (we already know it's 102 but let's do it properly).\n",
        "Then, we just allocate the first half to base categories and the remaining half to novel ones.\n",
        "We can do this because we are simulating a real world application, but keep in mind this will not happen out there!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nfq51vd8q_5a"
      },
      "outputs": [],
      "source": [
        "def base_novel_categories(dataset):\n",
        "    # set returns the unique set of all dataset classes\n",
        "    all_classes = set(dataset._labels)\n",
        "    # and let's count them\n",
        "    num_classes = len(all_classes)\n",
        "\n",
        "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
        "    # then we slice the list in half and generate base and novel category lists\n",
        "    base_classes = list(range(num_classes))[:num_classes//2]\n",
        "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
        "    return base_classes, novel_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvDdoYQr2fIu"
      },
      "source": [
        "## Inspect Classes\n",
        "Let's now visualize which are the base and novel classes.\n",
        "To do so, we first get a dummy test set (without augmentations) as we are just interested in the dataset labels. Then, we split it useing `base_novel_categories`.\n",
        "Finally, we use the hard-coded CLASS_NAMES to print the class in natural language.\n",
        "\n",
        "> Note: the list of class names was only recently added to `torchvision.datasets.Flowers102`. To avoid useless errors that can occour to you, we decided to also provide such a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veGGpNDctCgR",
        "outputId": "580bc9ef-b839-4ea8-8c32-29f54826f5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 345M/345M [00:16<00:00, 20.9MB/s]\n",
            "100%|██████████| 502/502 [00:00<00:00, 1.78MB/s]\n",
            "100%|██████████| 15.0k/15.0k [00:00<00:00, 9.71MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
            "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
          ]
        }
      ],
      "source": [
        "_, _, tmp_test = get_data()\n",
        "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
        "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8puO1VNpzwvi"
      },
      "source": [
        "## Split Dataset\n",
        "The next step is to actually split the dataset into the base and novel categories we extract from `base_novel_categories`.\n",
        "To split the data we need the dataset (obviously) and the list of base classes. If the sample label is not part of the base categories, then it must be part of the novel ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "msOszMs2zRRu"
      },
      "outputs": [],
      "source": [
        "def split_data(dataset, base_classes):\n",
        "    # these two lists will store the sample indexes\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "\n",
        "    # we create a set of base classes to compute the test below in O(1)\n",
        "    # this is optional and can be removed\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    # here we iterate over sample labels and also get the correspondent sample index\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    # here we create the dataset subsets\n",
        "    # the torch Subset is just a wrapper around the dataset\n",
        "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
        "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
        "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "    return base_dataset, novel_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQZT22rE8hBw"
      },
      "source": [
        "## Extract k shots\n",
        "As the dataset already provides 10 train and validation shots, we do not need to extract them.\n",
        "Beaware that Few-Shot Adaptation papers must do this operation as most datasets count significantly more samples in both the training and validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KpbPRLr7WL_"
      },
      "source": [
        "## Load CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh6uLZRT7YJx",
        "outputId": "61f16bf2-985e-42ee-d8a0-0166e2f774e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:13<00:00, 26.3MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    <function _convert_image_to_rgb at 0x78295c29c360>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "model = model.float()  # force the model to use float32\n",
        "\n",
        "# preprocess contains CLIP's pre-defined augmentations, let's inspect them!\n",
        "preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM9H14899ses"
      },
      "source": [
        "## Load and Prepare Data\n",
        "Here we get the three dataset split and pass clip pre-defined augmentations.\n",
        "Then, we compute base and novel categories (in this case is redundand as we already did it before).\n",
        "Finally, se split the three datasets into base and novel categories.\n",
        "As we want to use the novel categories only for the test set, we drop `train_novel` and `val_novel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TVrYUYTv9ttM"
      },
      "outputs": [],
      "source": [
        "# get the three datasets\n",
        "train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "\n",
        "# split classes into base and novel\n",
        "base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "# split the three datasets\n",
        "train_base, _ = split_data(train_set, base_classes)\n",
        "val_base, _ = split_data(val_set, base_classes)\n",
        "test_base, test_novel = split_data(test_set, base_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcgMwr3J9VIg"
      },
      "source": [
        "## Compute Zero-Shot Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7uhblkvm9US4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45dd88d-df7d-483f-8710-17cbcafe2db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🧠 Zero-shot evaluation on Base Classes: 100%|██████████| 20/20 [00:32<00:00,  1.63s/it]\n",
            "🧠 Zero-shot evaluation on Novel Classes: 100%|██████████| 29/29 [00:44<00:00,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Base classes accuracy: 71.29%\n",
            "🔍 Novel classes accuracy: 78.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad() # we don't want gradients\n",
        "def eval(model, dataset, categories, batch_size, device, label=\"\"):\n",
        "    # let's set the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Remap labels into a contiguous set starting from zero\n",
        "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
        "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
        "    text_inputs = clip.tokenize(\n",
        "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
        "    ).to(device)\n",
        "\n",
        "    # we can encode the text features once as they are shared for all images\n",
        "    # therefore we do it outside the evaluation loop\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "    # and here we normalize them (standard pratice with CLIP)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # simple dataloader creation\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "    for image, target in tqdm(dataloader, desc=label):\n",
        "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
        "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
        "        # Map targets in contiguous set starting from zero\n",
        "        # Labels needs to be .long() in pytorch\n",
        "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "        image = image.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # forward image through CLIP image encoder\n",
        "        image_features = model.encode_image(image)\n",
        "        # and normalize\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
        "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
        "        correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    # and now we compute the accuracy\n",
        "    accuracy = correct_predictions / len(dataset)\n",
        "    return accuracy\n",
        "\n",
        "base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Base Classes\")\n",
        "novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"🧠 Zero-shot evaluation on Novel Classes\")\n",
        "\n",
        "print()\n",
        "print(f\"🔍 Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "print(f\"🔍 Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baYfLKNdfbUR"
      },
      "source": [
        "## Harmonic Mean\n",
        "Few-Shot Adaptations papers usually report the Harmonic Mean.\n",
        "The harmonic mean tends to mitigate the impact of large outliers (base accuracy) and aggravate the impact of small ones (novel accuracy).\n",
        "Thus, achieving very high base accuracies at the expense of the novel accuracy will be penalized by the HM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rKAXR7hlfbUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3099d7-88a4-4e99-9817-5694d976e3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Harmonic Mean: 74.60%\n"
          ]
        }
      ],
      "source": [
        "def harmonic_mean(base_accuracy, novel_accuracy):\n",
        "    numerator = 2\n",
        "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
        "    hm = numerator / denominator\n",
        "    return hm\n",
        "\n",
        "print(f\"🔍 Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "oAdLnDy2dINF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large-scale vision–language models such as **CLIP** achieve remarkable *zero-shot* recognition by embedding images and text into a shared semantic space.  \n",
        "\n",
        "However, in **few-shot adaptation**—where base classes $\\mathcal{C}_b$ have labeled examples and novel classes $\\mathcal{C}_n$ are known only by name—CLIP remains unbalanced: it performs strongly on novel classes (thanks to its text priors) but struggles to fully exploit supervision on base classes.  \n",
        "\n",
        "We introduce a **Coupled Curved Text-to-Text Dictionary Transport (C2T2D)** framework that learns a **self-contained transport field** on CLIP’s **text manifold**, without relying on a zero-shot teacher.  \n",
        "The goal is to let each class anchor *adapt geometrically* to image evidence while preserving CLIP’s intrinsic structure.\n",
        "\n",
        "---\n",
        "\n",
        "- On the **text side**, we model CLIP’s embedding space as a **curved manifold**.  \n",
        "  Around each canonical class anchor $z_c$, we build a **tangent dictionary** $G_c$ by *mixing shared atlas dictionaries* through class-conditioned attention.  \n",
        "  Each $G_c$ spans valid semantic directions on the unit sphere—directions along which the meaning of a class can move without leaving CLIP’s manifold.\n",
        "\n",
        "- On the **image side**, the transport head predicts how each image embedding $x$ should **move its corresponding anchor** along those directions using a small **geodesic update**:\n",
        "  $$\n",
        "  z_c' = z_c \\cos s_c + u_c \\sin s_c,\n",
        "  $$\n",
        "  where $u_c$ is a predicted tangent direction and $s_c$ is a learned step size that controls the rotation magnitude.  \n",
        "  This keeps the motion smooth, localized, and consistent with CLIP’s geometry.\n",
        "\n",
        "- The head is trained **entirely from data**, without zero-shot distillation, using a **contrastive (InfoNCE)** objective combined with light geometric regularizers (consistency, span, and step penalties).  \n",
        "  These ensure stable, geometry-preserving transport that respects the structure of CLIP’s text manifold.\n",
        "\n",
        "---\n",
        "\n",
        "By learning to align images and class anchors through **geodesic motion** rather than fixed zero-shot templates,  \n",
        "C2T2D achieves a balanced improvement across **base** and **novel** classes.  \n",
        "The model maintains CLIP’s zero-shot robustness while introducing a *teacher-free, manifold-aware refinement* that optimizes the **harmonic mean (HM)** of both domains."
      ],
      "metadata": {
        "id": "KSDIsPkMdE9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phrase Bank and Text Encoders"
      ],
      "metadata": {
        "id": "-UMS8SX20vaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To capture linguistic variability, we build a **phrase bank** of diverse textual prompts for each class, such as  \n",
        "*“a close-up of {name}”*, *“a photo of blooming {name}”*, or *“a natural photo of {name}”*.  \n",
        "This exposes CLIP’s text encoder to a broad range of linguistic contexts rather than relying on a single rigid template.  \n",
        "\n",
        "Each phrase is encoded with CLIP’s text encoder $g_\\phi(\\cdot)$:\n",
        "\n",
        "$$\n",
        "z_{c,j} = g_\\phi(t_{c,j}), \\qquad\n",
        "\\hat{z}_{c,j} = \\frac{z_{c,j}}{\\|z_{c,j}\\|},\n",
        "$$\n",
        "\n",
        "where $t_{c,j}$ is the $j$-th textual variant for class $c$.  \n",
        "The normalized embeddings $\\hat{z}_{c,j}$ form a *cloud* of samples that locally describe the same concept under different phrasings.  \n",
        "\n",
        "From this cloud we define a **canonical class anchor** using a neutral template:\n",
        "\n",
        "$$\n",
        "z_c = g_\\phi(\\text{“a photo of a } c \\text{, a type of flower”}), \\qquad \\|z_c\\| = 1.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Building a shared semantic atlas\n",
        "To promote cross-class sharing of geometric structure, we aggregate *all* phrase embeddings across classes and cluster them into $M$ **semantic hubs** with centers $\\{a_m\\}_{m=1}^M$.  \n",
        "Each hub defines an **atlas anchor** $a_m$ and a local dictionary $G_m$ that captures common directions of variation (e.g., “macro shot,” “in a garden,” “color shift”).  \n",
        "\n",
        "Each class then forms its own **tangent basis** by softly mixing these shared dictionaries through class-conditioned attention:\n",
        "\n",
        "$$\n",
        "\\alpha_{c,m} = \\frac{\\exp(z_c^\\top a_m / \\tau)}{\\sum_{m'} \\exp(z_c^\\top a_{m'} / \\tau)}, \\qquad\n",
        "\\hat{G}_c = \\sum_{m=1}^M \\alpha_{c,m}\\, \\mathrm{Proj}_{T_{z_c}}(G_m),\n",
        "$$\n",
        "\n",
        "followed by QR re-orthonormalization to ensure orthogonality on the sphere.  \n",
        "The resulting $\\hat{G}_c$ serves as the **final tangent basis** for class $c$, combining both *local semantics* and *global manifold structure*.\n",
        "\n",
        "---\n",
        "\n",
        "**Intuition:**  \n",
        "- The phrase variants $\\hat{z}_{c,j}$ capture *semantic diversity* for each class.  \n",
        "- The atlas hubs $\\{a_m, G_m\\}$ capture *global patterns* of meaning shared across classes.  \n",
        "- The mixed tangent basis $\\hat{G}_c$ integrates both, defining *how the class concept can bend or vary* while staying faithful to CLIP’s manifold geometry.  \n",
        "\n",
        "This hybrid geometric representation allows the model to reason about **directions of meaning** on the manifold — enabling smooth, manifold-aware adaptation during training."
      ],
      "metadata": {
        "id": "30E7q4rz-zrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_phrase_variants(class_names, max_per_class=200, seed=0):\n",
        "    random.seed(seed)\n",
        "    TEMPLATES = [\n",
        "        \"a {name}\",\n",
        "        \"a close-up of {name}\",\n",
        "        \"a macro photo of {name}\",\n",
        "        \"a photo of {name} petals\",\n",
        "        \"a photo of blooming {name}\",\n",
        "        \"a photo of {name} in a garden\",\n",
        "        \"a botanical photo of {name}\",\n",
        "        \"a natural photo of {name}\",\n",
        "        \"a {name} flower\",\n",
        "        \"a {name} blossom\",\n",
        "    ]\n",
        "    phrases = {c: [] for c in class_names}\n",
        "    for c in class_names:\n",
        "        extras = [\n",
        "            f\"vibrant {c}\", f\"wild {c}\", f\"fresh {c}\", f\"single {c}\", f\"double {c}\",\n",
        "            f\"pink {c}\", f\"yellow {c}\", f\"white {c}\", f\"red {c}\", f\"purple {c}\"\n",
        "        ]\n",
        "        pool = TEMPLATES + [t.replace(\"{name}\", e) for e in extras for t in [\"a photo of {name}\"]]\n",
        "        gen = []\n",
        "        for _ in range(max_per_class):\n",
        "            t = random.choice(pool)\n",
        "            gen.append(t.format(name=c))\n",
        "        phrases[c] = list(dict.fromkeys(gen))[:max_per_class]\n",
        "    return phrases\n",
        "\n",
        "def _tokenize_batch(text_list, device):\n",
        "    return clip.tokenize(text_list).to(device)\n",
        "\n",
        "def encode_text_prompts(text_list, device):\n",
        "    tokenized = _tokenize_batch(text_list, device)\n",
        "    with torch.no_grad():\n",
        "        z = model.encode_text(tokenized)\n",
        "        z = z / z.norm(dim=-1, keepdim=True)\n",
        "    return z\n",
        "\n",
        "@torch.no_grad()\n",
        "def class_name_embeddings(class_names, device):\n",
        "    texts = [f\"a photo of a {c}, a type of flower.\" for c in class_names]\n",
        "    Z = encode_text_prompts(texts, device)\n",
        "    return Z"
      ],
      "metadata": {
        "id": "HR4Ep7CmWOZ-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tangent Space Utilities"
      ],
      "metadata": {
        "id": "VVtmVWCMWjCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since CLIP text embeddings are L2-normalized, all text features lie on the **unit hypersphere**  \n",
        "$S^{d-1} \\subset \\mathbb{R}^d$.  \n",
        "Linear operations in $\\mathbb{R}^d$ can distort this curved geometry, so to reason about local structure correctly,  \n",
        "we work in **tangent spaces** around each canonical text anchor $z_c$.\n",
        "\n",
        "---\n",
        "\n",
        "### Tangent space definition\n",
        "For a unit anchor $z_c$, its tangent space is\n",
        "$$\n",
        "T_{z_c} = \\{\\, v \\in \\mathbb{R}^d \\;|\\; v^\\top z_c = 0 \\,\\}.\n",
        "$$\n",
        "\n",
        "This space contains all directions orthogonal to $z_c$, corresponding to infinitesimal “moves” that stay on the surface of CLIP’s sphere.  \n",
        "It provides a local linearization of the text manifold near that concept.\n",
        "\n",
        "---\n",
        "\n",
        "### From phrase variants to local directions\n",
        "To characterize how a class concept can *vary semantically*,  \n",
        "we generate multiple text prompts describing the same class (e.g.,  \n",
        "“a photo of a rose,” “a close-up of a rose,” “a rose in a garden”).  \n",
        "Each prompt embedding $\\hat{z}_{c,j}$ is L2-normalized and projected onto $T_{z_c}$:\n",
        "\n",
        "$$\n",
        "\\tilde{z}_{c,j} = \\hat{z}_{c,j} - (\\hat{z}_{c,j}^\\top z_c)\\, z_c.\n",
        "$$\n",
        "\n",
        "Collecting all projected variants $\\tilde{z}_{c,j}$, we perform SVD:\n",
        "\n",
        "$$\n",
        "\\tilde{Z}_c = U\\,\\Sigma\\,V^\\top, \\quad G_c = V_{1:r}.\n",
        "$$\n",
        "\n",
        "The columns of $G_c \\in \\mathbb{R}^{d \\times r}$ form an **orthonormal local basis** of semantic variation around $z_c$.\n",
        "\n",
        "---\n",
        "\n",
        "### Building a shared semantic atlas\n",
        "While each $G_c$ describes local variation for its class, these directions are often redundant across classes  \n",
        "(e.g., “macro view,” “in a garden,” “color tone”).  \n",
        "To share structure, we cluster all prompt embeddings from every class into $M$ **semantic hubs** with centers $\\{a_m\\}_{m=1}^M$,  \n",
        "each defining an **atlas anchor** and its own local dictionary $G_m$.\n",
        "\n",
        "Every class then obtains its final tangent basis $\\hat{G}_c$ by **mixing** these shared atlas dictionaries through class-conditioned attention:\n",
        "\n",
        "$$\n",
        "\\alpha_{c,m} = \\frac{\\exp(z_c^\\top a_m / \\tau)}{\\sum_{m'} \\exp(z_c^\\top a_{m'} / \\tau)}, \\qquad\n",
        "\\hat{G}_c = \\mathrm{QR}\\!\\left(\\sum_{m=1}^M \\alpha_{c,m}\\, \\mathrm{Proj}_{T_{z_c}}(G_m)\\right),\n",
        "$$\n",
        "\n",
        "where the projection ensures each direction lies in $T_{z_c}$ and the QR step re-orthonormalizes them.  \n",
        "These $\\hat{G}_c$ are the **final text-side bases** used by the transport head.\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition\n",
        "- Each class anchor $z_c$ lies on CLIP’s unit sphere and represents a concept.  \n",
        "- Its mixed tangent basis $\\hat{G}_c$ provides valid *semantic directions* of motion, combining local prompt structure and shared atlas geometry.  \n",
        "- Restricting movement to these directions lets the model deform meaning smoothly and geometrically—  \n",
        "  preserving CLIP’s manifold structure while allowing flexible adaptation.\n",
        "\n",
        "This hierarchical construction makes the learned transport **manifold-aware** and **semantically shared**:  \n",
        "local per-class semantics align with global variation patterns captured across the atlas."
      ],
      "metadata": {
        "id": "UJP-4HGpKI7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def build_tangent_bases(class_names, device, anchors_per_class=200, r=12, seed=0):\n",
        "    phrases = build_phrase_variants(class_names, anchors_per_class, seed)\n",
        "    Z0 = class_name_embeddings(class_names, device)\n",
        "    bases = []\n",
        "    for i, cname in enumerate(class_names):\n",
        "        Zc = encode_text_prompts(phrases[cname], device)\n",
        "        z0 = Z0[i:i+1]\n",
        "        # project anchors to tangent plane at z0\n",
        "        Zc_tan = Zc - (Zc @ z0.T) * z0\n",
        "        U, S, Vh = torch.linalg.svd(Zc_tan, full_matrices=False)\n",
        "        T = Vh[:r].T.contiguous()\n",
        "        T = T - z0.T @ (z0 @ T)\n",
        "        Q, _ = torch.linalg.qr(T, mode='reduced')\n",
        "        bases.append(Q)\n",
        "    return Z0, torch.stack(bases, dim=0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _torch_kmeans(X, M=3, iters=15, seed=0):\n",
        "    \"\"\"Simple k-means clustering on L2-normalized embeddings (works on CPU/CUDA).\"\"\"\n",
        "    g = torch.Generator(device=X.device).manual_seed(42 + int(seed))\n",
        "    N = X.size(0)\n",
        "    assert N >= M, f\"k-means needs N(={N}) >= M(={M})\"\n",
        "\n",
        "    init_idx = torch.randint(0, N, (M,), generator=g, device=X.device)\n",
        "    centers = X[init_idx].clone()\n",
        "\n",
        "    for _ in range(iters):\n",
        "        # cosine k-means (since X is L2-normalized)\n",
        "        assign = (X @ centers.T).argmax(dim=1)\n",
        "        new_centers = []\n",
        "        for m in range(M):\n",
        "            mask = (assign == m)\n",
        "            if mask.any():\n",
        "                c = X[mask].mean(dim=0, keepdim=True)\n",
        "            else:\n",
        "                ridx = torch.randint(0, N, (1,), generator=g, device=X.device)\n",
        "                c = X[ridx]\n",
        "            c = c / c.norm(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "            new_centers.append(c)\n",
        "\n",
        "        centers = torch.cat(new_centers, dim=0)\n",
        "    return centers / centers.norm(dim=-1, keepdim=True).clamp_min(1e-8)\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_text_anchors_and_bank(class_names_all, anchors_per_class=400, K=12, M=3, device=\"cpu\", seed=0):\n",
        "    \"\"\"Cluster text embeddings into atlas anchors + per-cluster dictionaries\"\"\"\n",
        "    phrases_all = build_phrase_variants(class_names_all, anchors_per_class, seed)\n",
        "    Z_all = encode_text_prompts([t for c in class_names_all for t in phrases_all[c]], device)\n",
        "    A = _torch_kmeans(Z_all, M=M, iters=20, seed=seed)\n",
        "    assign = (Z_all @ A.T).argmax(dim=1)\n",
        "    G_bank = []\n",
        "    for m in range(M):\n",
        "        Zm = Z_all[assign == m] if (assign == m).any() else Z_all\n",
        "        U, S, Vh = torch.linalg.svd(Zm - Zm.mean(dim=0, keepdim=True), full_matrices=False)\n",
        "        Q, _ = torch.linalg.qr(Vh[:K].T.contiguous(), mode='reduced')\n",
        "        G_bank.append(Q)\n",
        "    return A, torch.stack(G_bank, dim=0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def class_tangent_mixture(Z0, A_centers, G_bank, tau=0.07):\n",
        "    \"\"\"Mix atlas dictionaries per class to form tangent dictionary (batched QR).\"\"\"\n",
        "    C, d = Z0.shape\n",
        "    M, _, K = G_bank.shape\n",
        "    att = (Z0 @ A_centers.T) / tau\n",
        "    alpha = torch.softmax(att, dim=-1)\n",
        "\n",
        "    I = torch.eye(d, device=Z0.device).unsqueeze(0)\n",
        "    Proj = I - Z0.unsqueeze(-1) @ Z0.unsqueeze(1)\n",
        "\n",
        "    # project each bank Gm to each class tangent space in batch\n",
        "    Gc_all = []\n",
        "    for m in range(M):\n",
        "        Gc = torch.einsum('cij,jk->cik', Proj, G_bank[m])\n",
        "        Q, _ = torch.linalg.qr(Gc, mode='reduced')\n",
        "        Gc_all.append(Q)\n",
        "    Gc_all = torch.stack(Gc_all, dim=1)\n",
        "\n",
        "    # mix by attention weights, then re-orthonormalize once\n",
        "    Gc_mix = (alpha.view(C, M, 1, 1) * Gc_all).sum(dim=1)\n",
        "    Q_mix, _ = torch.linalg.qr(Gc_mix, mode='reduced')\n",
        "    return Q_mix, alpha"
      ],
      "metadata": {
        "id": "kV6F0jEbWqOE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Prototypes"
      ],
      "metadata": {
        "id": "xBSN93aPWwhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While tangent dictionaries capture **semantic variation** from text prompts, they may not perfectly reflect how objects look in the dataset.  \n",
        "To complement them, we compute **image prototypes** from the few available training samples in the base classes.\n",
        "\n",
        "---\n",
        "\n",
        "#### Computing prototypes\n",
        "For each base class $c \\in \\mathcal{C}_b$, we encode its few-shot images with CLIP’s image encoder $f_\\theta$:\n",
        "\n",
        "$$\n",
        "u_i = f_\\theta(x_i), \\quad \\hat{u}_i = \\frac{u_i}{\\|u_i\\|}.\n",
        "$$\n",
        "\n",
        "We then cluster the embeddings $\\hat{u}_i$ belonging to class $c$ into $k$ groups and use their centers as **class prototypes**:\n",
        "\n",
        "$$\n",
        "p_{c,m} \\approx \\text{cluster center of } \\{ \\hat{u}_i : y_i = c \\}, \\quad m = 1, \\dots, k.\n",
        "$$\n",
        "\n",
        "Each $p_{c,m}$ is L2-normalized.\n",
        "\n",
        "---\n",
        "\n",
        "#### Projection into tangent space\n",
        "To compare prototypes with text anchors, we project each prototype into the tangent space at the canonical anchor $z_c$:\n",
        "\n",
        "$$\n",
        "q_{c,m} = p_{c,m} - (p_{c,m}^\\top z_c) z_c, \\quad\n",
        "\\hat{q}_{c,m} = \\frac{q_{c,m}}{\\|q_{c,m}\\|}.\n",
        "$$\n",
        "\n",
        "The set\n",
        "\n",
        "$$\n",
        "Q_c = \\{ \\hat{q}_{c,1}, \\dots, \\hat{q}_{c,k} \\}\n",
        "$$\n",
        "\n",
        "thus forms **prototype-guided tangent directions** for class $c$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Intuition\n",
        "- Text dictionaries $G_c$ provide **semantic directions** from linguistic variation (different ways humans describe the class).  \n",
        "- Prototypes $Q_c$ provide **visual directions** grounded in the actual data distribution.  \n",
        "- By projecting prototypes into the tangent at $z_c$, we ensure that the visual evidence is expressed as *local semantic variations* around the anchor.  \n",
        "\n",
        "Together, $G_c$ and $Q_c$ enrich the representation of each class, letting the model align **what we say about a class** (text) with **how it actually looks** (images)."
      ],
      "metadata": {
        "id": "veAvhN5VYy2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @torch.no_grad()\n",
        "# def class_multi_protos(train_subset, class_ids, clip_model, device, k=2, iters=10, seed=0):\n",
        "#     \"\"\"Compute k-means prototypes per class from few-shot set\"\"\"\n",
        "#     cont = {c:i for i,c in enumerate(class_ids)}\n",
        "#     feats_by_c = [[] for _ in class_ids]\n",
        "#     loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=False, num_workers=2)\n",
        "#     for imgs, labels in loader:\n",
        "#         x = clip_model.encode_image(imgs.to(device))\n",
        "#         x = x / x.norm(dim=-1, keepdim=True)\n",
        "#         for i, lab in enumerate(labels.tolist()):\n",
        "#             if lab in cont:\n",
        "#                 feats_by_c[cont[lab]].append(x[i].detach())\n",
        "#     Pk_list = []\n",
        "#     for xs in feats_by_c:\n",
        "#         if not xs:\n",
        "#             Pk_list.append(None); continue\n",
        "#         X = torch.stack(xs, 0)\n",
        "#         C = X[torch.randperm(X.size(0), device=device)[:k]]\n",
        "#         for _ in range(iters):\n",
        "#             assign = (X @ C.T).argmax(dim=1)\n",
        "#             newC = []\n",
        "#             for m in range(k):\n",
        "#                 xm = X[assign == m] if (assign == m).any() else X\n",
        "#                 c = xm.mean(0, keepdim=True)\n",
        "#                 newC.append(c / c.norm(dim=-1, keepdim=True))\n",
        "#             C = torch.cat(newC, dim=0)\n",
        "#         Pk_list.append(C)\n",
        "#     return Pk_list\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def build_Q_multi(Z0, Pk_list, k):\n",
        "#     \"\"\"Build tangent directions from prototypes toward text anchors\"\"\"\n",
        "#     C, d = Z0.shape\n",
        "#     Qk = torch.zeros(C, k, d, device=Z0.device)\n",
        "#     mask = torch.zeros(C, dtype=torch.bool, device=Z0.device)\n",
        "#     for j in range(C):\n",
        "#         if Pk_list[j] is None: continue\n",
        "#         P = Pk_list[j]\n",
        "#         z = Z0[j:j+1]\n",
        "#         q = P - (P * z).sum(dim=-1, keepdim=True) * z\n",
        "#         q = q / (q.norm(dim=-1, keepdim=True).clamp_min(1e-8))\n",
        "\n",
        "#         Qk[j, :q.size(0)] = q; mask[j] = True\n",
        "#     return Qk, mask"
      ],
      "metadata": {
        "id": "sxKWcxpRW1_H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Head"
      ],
      "metadata": {
        "id": "8lp5KqDlW9ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have:\n",
        "- **Canonical text anchors** $z_c \\in \\mathbb{R}^d$ (one per class),\n",
        "- **Mixed tangent dictionaries** $G_c \\in \\mathbb{R}^{d \\times K}$, obtained by combining shared atlas directions specific to each class,\n",
        "\n",
        "we introduce the **C2T2D Transport Head** —  \n",
        "a lightweight module that predicts, for each image–class pair, a small **tangent-space displacement** and a **step size** describing how far to move along CLIP’s text manifold.  \n",
        "This produces a *transported version* of each class anchor that better aligns with the image while remaining on the unit sphere.\n",
        "\n",
        "---\n",
        "\n",
        "### Input features\n",
        "For every image embedding $x \\in \\mathbb{R}^d$ and class anchor $z_c$:\n",
        "\n",
        "- **Cosine similarity:** $s = x^\\top z_c$\n",
        "- **Chordal distance:** $1 - s$\n",
        "\n",
        "We concatenate these with both embeddings to form the feature vector:\n",
        "\n",
        "$$\n",
        "h_{x,c} = [x,\\, z_c,\\, s,\\, 1-s] \\in \\mathbb{R}^{2d + 2}.\n",
        "$$\n",
        "\n",
        "This representation captures both semantic and geometric context for each image–class pair.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Predicting the tangent-space direction\n",
        "A multilayer perceptron (MLP) takes $h_{x,c}$ and predicts coefficients $\\beta_{c,k}$ over the $K$ tangent directions of $G_c$:\n",
        "\n",
        "$$\n",
        "\\beta_c = \\text{MLP}_t(h_{x,c}) \\in \\mathbb{R}^K.\n",
        "$$\n",
        "\n",
        "These coefficients combine the tangent basis vectors $G_c = [g_{c,1}, \\dots, g_{c,K}]$ into a single **semantic motion**:\n",
        "\n",
        "$$\n",
        "u_c'(x) = \\sum_{k=1}^K \\beta_{c,k}\\, g_{c,k}.\n",
        "$$\n",
        "\n",
        "To ensure the motion stays on the sphere, we project it onto the tangent space of $z_c$ and re-normalize:\n",
        "\n",
        "$$\n",
        "u_c(x) = \\frac{u_c'(x) - (u_c'(x)^\\top z_c)\\,z_c}{\\|\\,u_c'(x) - (u_c'(x)^\\top z_c)\\,z_c\\|}.\n",
        "$$\n",
        "\n",
        "This gives a valid unit direction $u_c(x) \\in T_{z_c}$ —  \n",
        "the direction in which the class anchor should move to align with $x$.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Predicting the step size\n",
        "A second MLP predicts how far to move along this direction:\n",
        "\n",
        "$$\n",
        "s_c(x) = \\varepsilon_t \\cdot \\mathrm{softplus}\\!\\big(\\text{MLP}_s(h_{x,c})\\big), \\qquad \\varepsilon_t = 0.25.\n",
        "$$\n",
        "\n",
        "The softplus ensures positivity, and the cap $\\varepsilon_t$ keeps motion small and stable.  \n",
        "Unlike earlier versions, this step is **not similarity-gated** — the model learns its own step magnitude per pair.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Geodesic update on the unit sphere\n",
        "Using $s_c(x)$ and $u_c(x)$, we perform a **geodesic move** — the natural rotation that keeps points on the sphere:\n",
        "\n",
        "$$\n",
        "\\tilde{z}_c(x) = \\cos(s_c)\\, z_c + \\sin(s_c)\\, u_c(x),\n",
        "\\qquad \\|\\tilde{z}_c(x)\\| = 1.\n",
        "$$\n",
        "\n",
        "This produces a **transported anchor** $\\tilde{z}_c(x)$ that remains normalized and geometrically valid.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Classification logits\n",
        "We score each class by its similarity to the image after transport:\n",
        "\n",
        "$$\n",
        "\\ell_c(x) = (x^\\top \\tilde{z}_c(x)) \\cdot \\exp(\\gamma),\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is CLIP’s learned logit scale.  \n",
        "These logits feed into a **contrastive (InfoNCE)** loss during training.\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition\n",
        "- The **mixed tangent bases** $G_c$ capture how a class’s meaning can bend locally within the shared atlas of CLIP’s text manifold.  \n",
        "- The **transport head** predicts *which way to bend* (via $\\beta_c$) and *how far* (via $s_c$).  \n",
        "- The **geodesic move** preserves the manifold geometry, ensuring the anchor remains valid.  \n",
        "- The process refines text–image alignment without altering CLIP’s overall structure, maintaining its zero-shot generalization.\n",
        "\n",
        "In essence, the head learns a **local, geometry-preserving navigation rule**:  \n",
        "for each image, move each text anchor slightly in the right direction — always staying on the manifold."
      ],
      "metadata": {
        "id": "cDhlTD98eqDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class C2T2D_Head(nn.Module):\n",
        "    \"\"\"\n",
        "    Teacher-free geodesic transport head:\n",
        "      - predicts basis mix beta (over Gc)\n",
        "      - predicts step size s >= 0\n",
        "      - outputs transported anchors W and logits against x_img\n",
        "      - returns (logits, W, u, s, beta) for regularizers\n",
        "    \"\"\"\n",
        "    def __init__(self, d, K_text, Cb=0):\n",
        "        super().__init__()\n",
        "        self.bias_c = nn.Parameter(torch.zeros(Cb)) if Cb > 0 else None\n",
        "        in_dim = 2*d + 2\n",
        "\n",
        "        self.dir_mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, 256), nn.ReLU(), nn.Dropout(0.10),\n",
        "            nn.Linear(256, 256), nn.ReLU(), nn.Dropout(0.10),\n",
        "            nn.Linear(256, K_text)\n",
        "        )\n",
        "        self.step_mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 1)  # will softplus -> [0, +)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_img, Z0, Gc):\n",
        "        \"\"\"\n",
        "        x_img: [B,d] normalized\n",
        "        Z0:    [C,d] class anchors (normalized)\n",
        "        Gc:    [C,d,K] orthonormal tangent bases per class\n",
        "        \"\"\"\n",
        "        B, d = x_img.shape\n",
        "        C, d2, K = Gc.shape\n",
        "        assert d == d2\n",
        "\n",
        "        # tile tensors\n",
        "        X = x_img.unsqueeze(1).expand(B, C, d)\n",
        "        Z = Z0.unsqueeze(0).expand(B, C, d)\n",
        "\n",
        "        # simple image-text features for the head\n",
        "        xdot = (X * Z).sum(-1, keepdim=True)\n",
        "        dist2 = 1.0 - xdot\n",
        "        h_in = torch.cat([X, Z, xdot, dist2], dim=-1)\n",
        "\n",
        "        # predict basis mix and step\n",
        "        beta = self.dir_mlp(h_in)\n",
        "        s = F.softplus(self.step_mlp(h_in))\n",
        "        s = 0.25 * s\n",
        "\n",
        "        # build unit tangent direction u from class basis\n",
        "        GcBC = Gc.unsqueeze(0).expand(B, -1, -1, -1)\n",
        "        u = (beta.unsqueeze(-2) @ GcBC.transpose(-2, -1)).squeeze(-2)\n",
        "        u = u - (u * Z).sum(-1, keepdim=True) * Z\n",
        "        u = u / (u.norm(dim=-1, keepdim=True).clamp_min(1e-8))\n",
        "\n",
        "        # geodesic exponential map on the unit sphere\n",
        "        zcos = torch.cos(s)\n",
        "        zsin = torch.sin(s)\n",
        "        Zp = Z * zcos + u * zsin\n",
        "        W = F.normalize(Zp, dim=-1)\n",
        "\n",
        "        # logits against images\n",
        "        logits = torch.einsum('bd,bcd->bc', x_img, W) * model.logit_scale.exp()\n",
        "        if self.bias_c is not None:\n",
        "            logits[:, :self.bias_c.numel()] += self.bias_c\n",
        "\n",
        "        return logits, W, u, s, beta"
      ],
      "metadata": {
        "id": "lddiNtsRXBqu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why This Classifier Works\n",
        "\n",
        "The classifier performs **manifold-constrained alignment** between image and text embeddings.  \n",
        "Rather than arbitrarily warping CLIP’s space, it learns to move **along** the intrinsic geometry of the unit hypersphere —  \n",
        "adjusting class anchors through small, valid rotations within their tangent spaces.\n",
        "\n",
        "This operation admits two equivalent geometric interpretations:\n",
        "\n",
        "- **Image-side view:** the image embedding $x$ is *transported* toward its most compatible class anchor $z_c$  \n",
        "  along directions that are valid within CLIP’s text manifold.\n",
        "- **Text-side view (used in practice):** the class anchor $z_c$ is *locally transported* toward the image inside its own tangent space.\n",
        "\n",
        "Both describe the **same geodesic rotation** on the sphere; only the frame of reference differs.\n",
        "\n",
        "---\n",
        "\n",
        "If $x$ **belongs to class $c$**, the model predicts a small motion that follows the low-curvature structure around $z_c$,  \n",
        "bringing the transported anchor $\\tilde{z}_c(x)$ closer to $x$ and producing a high cosine score.\n",
        "\n",
        "If $x$ **does not belong to class $c$**, any movement allowed by the tangent-space basis fails to align with $x$,  \n",
        "since the manifold itself restricts how far the anchor can bend — the similarity therefore remains low.\n",
        "\n",
        "Thus, instead of asking:\n",
        "\n",
        "$$\n",
        "\\text{“Which fixed anchor is closest to } x \\text{?”}\n",
        "$$\n",
        "\n",
        "the classifier now asks:\n",
        "\n",
        "$$\n",
        "\\text{“Which class anchor can align with } x \\text{through a valid geodesic move on the manifold?”}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Why This Helps\n",
        "\n",
        "- The model is **manifold-aware** — every update is a geodesic motion that preserves CLIP’s spherical structure.  \n",
        "- Each anchor **adapts semantically** through its tangent directions without leaving the manifold.  \n",
        "- **Base and novel classes** share this same geometric mechanism, enabling consistent generalization.  \n",
        "- The system balances **semantic flexibility** (moving along valid directions) and **geometric stability** (staying on the sphere).\n",
        "\n",
        "---\n",
        "\n",
        "In short:  \n",
        "classification becomes a search for the **most geometrically plausible alignment** between an image and each class anchor —  \n",
        "where all motion is confined to **legitimate directions of meaning** on CLIP’s text manifold."
      ],
      "metadata": {
        "id": "O5avau4KoV_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Helpers"
      ],
      "metadata": {
        "id": "ugcobOXLXKqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def few_shot_subset(dataset, base_class_indices, shots=10, seed=0):\n",
        "    rng = random.Random(seed)\n",
        "    indices_by_class = defaultdict(list)\n",
        "    if isinstance(dataset, torch.utils.data.Subset):\n",
        "        for i, orig_idx in enumerate(dataset.indices):\n",
        "            label = dataset.dataset._labels[orig_idx]\n",
        "            if label in base_class_indices:\n",
        "                indices_by_class[label].append(orig_idx)\n",
        "    else:\n",
        "        for i, label in enumerate(dataset._labels):\n",
        "            if label in base_class_indices:\n",
        "                indices_by_class[label].append(i)\n",
        "\n",
        "    selected = []\n",
        "    for c in base_class_indices:\n",
        "        pool = indices_by_class[c]\n",
        "        rng.shuffle(pool)\n",
        "        selected.extend(pool[:shots])\n",
        "\n",
        "    return torch.utils.data.Subset(\n",
        "        dataset.dataset if isinstance(dataset, torch.utils.data.Subset) else dataset,\n",
        "        selected\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_kpb(head, class_ids, Z0, Gc, Qk, dataset, desc, T_star=1.0):\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "    cont = {orig:i for i, orig in enumerate(class_ids)}\n",
        "    ok = tot = 0\n",
        "    for imgs, lbls in tqdm(loader, desc=desc):\n",
        "        imgs = imgs.to(device)\n",
        "        y = torch.tensor([cont[l.item()] for l in lbls], device=device)\n",
        "        x = model.encode_image(imgs); x = x / x.norm(dim=-1, keepdim=True)\n",
        "        logits, _, _, _, _ = head(x, Z0, Gc)\n",
        "        pred = (logits / T_star).argmax(1)\n",
        "        ok += (pred == y).sum().item(); tot += imgs.size(0)\n",
        "    return ok / max(1, tot)"
      ],
      "metadata": {
        "id": "-6XW8FHyXN82"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "SW1mQ3MIXSDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42); np.random.seed(42); random.seed(42)\n",
        "\n",
        "base_names = [CLASS_NAMES[i] for i in base_classes]\n",
        "novel_names = [CLASS_NAMES[i] for i in novel_classes]\n",
        "A_centers, G_bank = build_text_anchors_and_bank(base_names, anchors_per_class=400, K=20, M=2, device=device)\n",
        "Z0_base = class_name_embeddings(base_names,  device)\n",
        "Z0_novel = class_name_embeddings(novel_names, device)\n",
        "Gc_base_mix, _ = class_tangent_mixture(Z0_base,  A_centers, G_bank, tau=0.07)\n",
        "Gc_novel_mix, _ = class_tangent_mixture(Z0_novel, A_centers, G_bank, tau=0.07)\n",
        "\n",
        "# build few-shot subset for base\n",
        "train_base_full, _ = split_data(train_set, base_classes)\n",
        "train_10shot = few_shot_subset(train_base_full, base_classes, shots=10, seed=42)\n",
        "g = torch.Generator().manual_seed(42)\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_10shot,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    generator=g\n",
        ")\n",
        "\n",
        "# base prototypes\n",
        "# K_PROTO = 3\n",
        "# Pk_base = class_multi_protos(train_10shot, base_classes, model, device, k=K_PROTO, iters=10)\n",
        "# Qk_base, _ = build_Q_multi(Z0_base, Pk_base, k=K_PROTO)\n",
        "# Qk_novel = torch.zeros(Z0_novel.size(0), K_PROTO, Z0_novel.size(1), device=device)"
      ],
      "metadata": {
        "id": "ZXbbLzMrXUPY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Few-Shot (FSA) Predictions"
      ],
      "metadata": {
        "id": "54pxxeeCXX7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model architecture is in place, we train the transport head using a mix of **contrastive, geometric, and stabilization losses**.  \n",
        "The goal is to learn a smooth, manifold-aware transport field that improves alignment on base classes while preserving CLIP’s zero-shot structure for novel ones.\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. Contrastive Cross-Entropy (InfoNCE)\n",
        "For each batch, we compute logits $\\ell_c(x)$ between image embeddings $x$ and their transported text anchors $\\tilde{z}_c(x)$.  \n",
        "As in CLIP, we apply a temperature-scaled cross-entropy loss:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{ctr} = \\text{CrossEntropy}\\!\\left( \\frac{\\ell(x)}{T_{\\text{InfoNCE}}},\\, y \\right).\n",
        "$$\n",
        "\n",
        "This is the **primary training signal**, encouraging each image to align most strongly with its correct class after geodesic transport.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Dropout Consistency\n",
        "To promote stability, the head is evaluated twice per batch with dropout active, producing two sets of transported anchors $W$ and $W'$.  \n",
        "We enforce consistency between them:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{cons} = \\big(1 - \\cos(W, W')\\big)_{\\text{mean}}.\n",
        "$$\n",
        "\n",
        "This regularizer reduces stochastic variance and encourages smooth atlas mixtures across dropout perturbations.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Span Regularization\n",
        "To ensure the predicted motion $u_c(x)$ stays within each class’s valid tangent subspace (spanned by its mixed basis $G_c$),  \n",
        "we penalize any component that lies outside that subspace:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{span} = \\|u - P_{G_c}u\\|_2^2,\n",
        "$$\n",
        "\n",
        "where $P_{G_c} = G_c G_c^\\top$ is the projection matrix computed from the current bases.  \n",
        "This keeps the learned transport **geometry-consistent**, ensuring movement occurs only along valid manifold directions.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Small-Step Prior\n",
        "To maintain local linearity and avoid overfitting, we penalize large step sizes:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{step} = \\|s\\|_2^2.\n",
        "$$\n",
        "\n",
        "This term encourages smooth, interpretable rotations on the sphere rather than abrupt reorientations.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. Weak Zero-Shot Tether\n",
        "Although the head trains without any zero-shot teacher, we add a light **hinge tether** that prevents transported logits from dropping too far below CLIP’s original zero-shot scores:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{tether} = \\max\\!\\big(0,\\, (\\ell_{\\text{ZS}} - \\ell) - \\delta\\big),\n",
        "$$\n",
        "\n",
        "where $\\delta$ is a small safety margin (e.g., 0.02).  \n",
        "This term does *not* introduce gradients from CLIP itself—it simply prevents the model from degrading its prior knowledge.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. Total Loss\n",
        "The final training objective is a weighted combination:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathcal{L} =&\\;\n",
        "\\mathcal{L}_{ctr}\n",
        "+ 0.3\\, \\mathcal{L}_{cons}\n",
        "+ 0.1\\, \\mathcal{L}_{span}\n",
        "+ 0.05\\, \\mathcal{L}_{step} \\\\\n",
        "&+ 0.05\\, \\mathcal{L}_{tether}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "These coefficients are empirically tuned for stability and transfer balance.\n",
        "\n",
        "---\n",
        "\n",
        "#### Intuition\n",
        "- **Contrastive loss** enforces correct alignment.  \n",
        "- **Consistency** stabilizes stochastic behavior.  \n",
        "- **Span** and **step** regularizers keep the motion geometrically valid and small.  \n",
        "- The **ZS tether** prevents catastrophic forgetting of CLIP’s zero-shot prior.\n",
        "\n",
        "Together, these losses teach the head to learn a **smooth, geometry-preserving transport field**—  \n",
        "refining base classes through data-driven alignment while generalizing gracefully to unseen novel ones."
      ],
      "metadata": {
        "id": "CWSm_J5Cpet4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = str(device).startswith(\"cuda\")\n",
        "\n",
        "d = Z0_base.size(1); K_text = Gc_base_mix.size(2)\n",
        "head = C2T2D_Head(d, K_text).to(device)\n",
        "\n",
        "EPOCHS = 20\n",
        "opt = torch.optim.AdamW(head.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    opt, max_lr=2e-3, steps_per_epoch=len(train_loader), epochs=EPOCHS, pct_start=0.3\n",
        ")\n",
        "\n",
        "T_INFONCE = 0.10\n",
        "contig = {orig: i for i, orig in enumerate(base_classes)}\n",
        "best_hm, best_state = -1.0, None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    head.train(); tot = n = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        y = torch.tensor([contig[l.item()] for l in labels], device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.amp.autocast(device_type=\"cuda\", enabled=use_cuda, dtype=torch.float16):\n",
        "                x = model.encode_image(images)\n",
        "            x = F.normalize(x.float(), dim=-1)\n",
        "\n",
        "        # Forward: teacher-free transport\n",
        "        logits, W, u, s, beta = head(x, Z0_base, Gc_base_mix)\n",
        "\n",
        "        # -------- Losses ----------\n",
        "        # 1) InfoNCE (image -> transported text), classic CE over classes\n",
        "        loss_ctr = F.cross_entropy(logits / T_INFONCE, y)\n",
        "\n",
        "        # 2) Simple dropout-consistency: run the head twice and align W\n",
        "        if head.training:\n",
        "            logits2, W2, _, _, _ = head(x, Z0_base, Gc_base_mix)\n",
        "            loss_cons = (1.0 - (W * W2).sum(dim=-1)).mean()  # 1 - cosine since W are unit\n",
        "        else:\n",
        "            loss_cons = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # 3) Span regularizer built from the CURRENT Gc used in forward\n",
        "        Gc_used = Gc_base_mix\n",
        "        PB = (Gc_used @ Gc_used.transpose(1, 2))\n",
        "        PB = PB.unsqueeze(0).expand(x.size(0), -1, -1, -1)\n",
        "        u_proj = (PB @ u.unsqueeze(-1)).squeeze(-1)\n",
        "        loss_span = ((u - u_proj)**2).mean()\n",
        "\n",
        "        # 4) Small-step prior\n",
        "        loss_step = (s**2).mean()\n",
        "\n",
        "        # 5) Weak ZS tether (hinge so we don't tank below ZS by a lot)\n",
        "        with torch.no_grad():\n",
        "            zs_logits = (x @ Z0_base.T) * model.logit_scale.exp()\n",
        "        margin = 0.02\n",
        "        loss_tether = F.relu(zs_logits - logits - margin).mean()\n",
        "\n",
        "        # Weights (these are educated guesses, need finetuning)\n",
        "        loss = (\n",
        "            loss_ctr\n",
        "            + 0.3 * loss_cons\n",
        "            + 0.1 * loss_span\n",
        "            + 0.05 * loss_step\n",
        "            + 0.05 * loss_tether\n",
        "        )\n",
        "        # --------------------------\n",
        "\n",
        "        opt.zero_grad(); loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(head.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        tot += loss.item() * images.size(0); n += images.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: loss={(tot/n):.4f}\")\n",
        "\n",
        "# eval + checkpoint\n",
        "head.eval()\n",
        "b = eval_kpb(head, base_classes,  Z0_base,  Gc_base_mix,  None, test_base,  \"C2T2D Base\")\n",
        "nv = eval_kpb(head, novel_classes, Z0_novel, Gc_novel_mix, None, test_novel, \"C2T2D Novel\")\n",
        "hm = 2 / (1 / b + 1 / nv)\n",
        "print(f\"[C2T2D] Base={b*100:.2f}% Novel={nv*100:.2f}% HM={hm*100:.2f}%\")\n",
        "if hm > best_hm:\n",
        "    best_hm, best_state = hm, {k: v.detach().cpu().clone() for k, v in head.state_dict().items()}\n",
        "    print(\"   -> saved best C2T2D-HM checkpoint\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV7yZaekXus3",
        "outputId": "2bbd81e6-f982-4add-a71c-bfc6bc848f5b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: loss=9.1639\n",
            "Epoch 2: loss=8.2000\n",
            "Epoch 3: loss=6.7442\n",
            "Epoch 4: loss=4.1904\n",
            "Epoch 5: loss=2.5762\n",
            "Epoch 6: loss=1.6945\n",
            "Epoch 7: loss=1.5890\n",
            "Epoch 8: loss=0.9780\n",
            "Epoch 9: loss=0.6563\n",
            "Epoch 10: loss=0.5348\n",
            "Epoch 11: loss=0.4239\n",
            "Epoch 12: loss=0.3910\n",
            "Epoch 13: loss=0.2394\n",
            "Epoch 14: loss=0.1960\n",
            "Epoch 15: loss=0.1441\n",
            "Epoch 16: loss=0.0707\n",
            "Epoch 17: loss=0.1120\n",
            "Epoch 18: loss=0.0557\n",
            "Epoch 19: loss=0.0627\n",
            "Epoch 20: loss=0.0471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C2T2D Base: 100%|██████████| 20/20 [00:30<00:00,  1.51s/it]\n",
            "C2T2D Novel: 100%|██████████| 29/29 [00:43<00:00,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[C2T2D] Base=90.17% Novel=76.82% HM=82.96%\n",
            "   -> saved best C2T2D-HM checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Evaluation"
      ],
      "metadata": {
        "id": "ekINUCjo6G33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def _encode_imgs(imgs, device):\n",
        "    with torch.amp.autocast(device_type=\"cuda\", enabled=str(device).startswith(\"cuda\"), dtype=torch.float16):\n",
        "        x = model.encode_image(imgs.to(device))\n",
        "    return F.normalize(x.float(), dim=-1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_zs(dataset, class_ids, Z0, batch=128, name=\"ZS\", device=\"cuda\"):\n",
        "    cont = {orig:i for i, orig in enumerate(class_ids)}\n",
        "    loader = DataLoader(dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
        "    ok = tot = 0\n",
        "    for imgs, lbls in loader:\n",
        "        y = torch.tensor([cont[l.item()] for l in lbls], device=device)\n",
        "        x = _encode_imgs(imgs, device)\n",
        "        logits = (x @ Z0.T) * model.logit_scale.exp()\n",
        "        pred = logits.argmax(1)\n",
        "        ok += (pred == y).sum().item(); tot += y.size(0)\n",
        "    acc = ok / max(1, tot)\n",
        "    print(f\"[{name}] Acc={acc*100:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_head_only(dataset, class_ids, Z0, Gc, head, batch=128, name=\"HEAD\", device=\"cuda\"):\n",
        "    cont = {orig:i for i, orig in enumerate(class_ids)}\n",
        "    loader = DataLoader(dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
        "    ok = tot = 0\n",
        "    for imgs, lbls in loader:\n",
        "        y = torch.tensor([cont[l.item()] for l in lbls], device=device)\n",
        "        x = _encode_imgs(imgs, device)\n",
        "        logits, _, _, _, _ = head(x, Z0, Gc)\n",
        "        pred = logits.argmax(1)\n",
        "        ok += (pred == y).sum().item(); tot += y.size(0)\n",
        "    acc = ok / max(1, tot)\n",
        "    print(f\"[{name}] Acc={acc*100:.2f}%\")\n",
        "    return acc\n",
        "\n",
        "@torch.no_grad()\n",
        "def compare_agreement(dataset, class_ids, Z0, Gc, head, batch=128, name=\"Compare\", device=\"cuda\"):\n",
        "    cont = {orig:i for i, orig in enumerate(class_ids)}\n",
        "    loader = DataLoader(dataset, batch_size=batch, shuffle=False, num_workers=2)\n",
        "    agree = head_wins = zs_wins = tot = 0\n",
        "    for imgs, lbls in loader:\n",
        "        y = torch.tensor([cont[l.item()] for l in lbls], device=device)\n",
        "        x = _encode_imgs(imgs, device)\n",
        "        zs = (x @ Z0.T) * model.logit_scale.exp()\n",
        "        hd, _, _, _, _ = head(x, Z0, Gc)\n",
        "        p_zs = zs.argmax(1); p_hd = hd.argmax(1)\n",
        "        agree += (p_zs == p_hd).sum().item()\n",
        "        head_wins += ((p_hd == y) & (p_zs != y)).sum().item()\n",
        "        zs_wins   += ((p_zs == y) & (p_hd != y)).sum().item()\n",
        "        tot += y.size(0)\n",
        "    print(f\"[{name}] agree={agree/tot*100:.1f}% | head>zs={head_wins} | zs>head={zs_wins} | total={tot}\")\n",
        "\n",
        "print(\"== BASE ==\")\n",
        "acc_zs_base = eval_zs(test_base,  base_classes,  Z0_base,  name=\"ZS-Base\",   device=device)\n",
        "acc_head_base = eval_head_only(test_base, base_classes, Z0_base, Gc_base_mix, head, name=\"HEAD-Base\", device=device)\n",
        "compare_agreement(test_base, base_classes, Z0_base, Gc_base_mix, head, name=\"Base agreement\", device=device)\n",
        "\n",
        "print(\"\\n== NOVEL ==\")\n",
        "acc_zs_novel = eval_zs(test_novel,  novel_classes,  Z0_novel,  name=\"ZS-Novel\",   device=device)\n",
        "acc_head_novel = eval_head_only(test_novel, novel_classes, Z0_novel, Gc_novel_mix, head, name=\"HEAD-Novel\", device=device)\n",
        "compare_agreement(test_novel, novel_classes, Z0_novel, Gc_novel_mix, head, name=\"Novel agreement\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAfgwY7O6Ex8",
        "outputId": "f31bc1fa-fc2a-4d3b-a0ed-9ae1eb7476bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== BASE ==\n",
            "[ZS-Base] Acc=71.29%\n",
            "[HEAD-Base] Acc=90.25%\n",
            "[Base agreement] agree=72.0% | head>zs=551 | zs>head=82 | total=2473\n",
            "\n",
            "== NOVEL ==\n",
            "[ZS-Novel] Acc=78.24%\n",
            "[HEAD-Novel] Acc=76.82%\n",
            "[Novel agreement] agree=90.8% | head>zs=51 | zs>head=103 | total=3676\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "2353MHw1p24h",
        "KJI_a5EizA5a",
        "kvDdoYQr2fIu",
        "8puO1VNpzwvi",
        "-KpbPRLr7WL_",
        "lM9H14899ses",
        "xBSN93aPWwhY"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}